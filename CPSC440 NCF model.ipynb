{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sample Neural Collaborative Filtering model\n\nBased on He et al. paper and a tutorial based on it foudn at https://www.kaggle.com/code/curiousraccoon/deep-learning-based-recommender-systems/edit.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv\n\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\n\nnp.random.seed(123)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-24T23:35:23.463453Z","iopub.execute_input":"2022-04-24T23:35:23.463902Z","iopub.status.idle":"2022-04-24T23:35:26.912644Z","shell.execute_reply.started":"2022-04-24T23:35:23.463870Z","shell.execute_reply":"2022-04-24T23:35:26.911811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_pop_items = 15\none_month_before_val_date = '2020-08-15'\nneg_sample_num = 3\ndays_max_diff = 14\ntrain_begin_date = \"2020-03-15\"\ntrain_end_date = \"2020-09-15\"\nval_end_date = \"2020-09-22\"","metadata":{"execution":{"iopub.status.busy":"2022-04-25T02:41:59.829360Z","iopub.execute_input":"2022-04-25T02:41:59.830087Z","iopub.status.idle":"2022-04-25T02:41:59.836634Z","shell.execute_reply.started":"2022-04-25T02:41:59.830046Z","shell.execute_reply":"2022-04-25T02:41:59.835937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"def load_data_frames():\n    # load csv files\n    data_path = '../input/h-and-m-personalized-fashion-recommendations/'\n    csv_train = f'{data_path}transactions_train.csv'\n    csv_sub = f'{data_path}sample_submission.csv'\n    csv_users = f'{data_path}customers.csv'\n    csv_items = f'{data_path}articles.csv'\n\n    df = pd.read_csv(csv_train, dtype={'article_id': str}, parse_dates=['t_dat'])\n    df_sub = pd.read_csv(csv_sub)\n    dfu = pd.read_csv(csv_users)\n    dfi = pd.read_csv(csv_items, dtype={'article_id': str})\n    \n    return df, df_sub, dfu, dfi\n\n# df - transaction dataframe\n# df_sub - submission datafram\n# dfu - customers dataframe\n# dfi - articles dataframe\n\ndf, df_sub, dfu, dfi = load_data_frames()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:15:31.306088Z","iopub.execute_input":"2022-04-25T04:15:31.306871Z","iopub.status.idle":"2022-04-25T04:16:47.073265Z","shell.execute_reply.started":"2022-04-25T04:15:31.306818Z","shell.execute_reply":"2022-04-25T04:16:47.072468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the dictionary\ndef create_item_group_dict():\n    group_dict = {}\n    groups = dfi.groupby(['index_name', 'product_group_name', 'section_name', 'product_type_name']).groups\n\n    for key, value in groups.items():\n        value = list(value)\n        index_name = key[0]\n        product_group_name = key[1]\n        section_name = key[2]\n        product_type_name = key[3]\n    \n        if not (index_name in group_dict):\n            group_dict[index_name] = {\n                product_group_name: {\n                    section_name: {\n                        product_type_name: value\n                    }\n                }\n            }\n    \n        elif not (product_group_name in group_dict[index_name]):\n            group_dict[index_name][product_group_name] ={\n                section_name: {\n                    product_type_name: value\n                }\n            }\n\n        elif not (section_name in group_dict[index_name][product_group_name]):\n            group_dict[index_name][product_group_name][section_name] = {\n                product_type_name: value\n            }\n        else:\n            group_dict[index_name][product_group_name][section_name][product_type_name] = value\n            \n    return group_dict\n\ngroup_dict = create_item_group_dict()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:38:50.103849Z","iopub.execute_input":"2022-04-24T23:38:50.104105Z","iopub.status.idle":"2022-04-24T23:38:50.427476Z","shell.execute_reply.started":"2022-04-24T23:38:50.104078Z","shell.execute_reply":"2022-04-24T23:38:50.426775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# modify the above dictionary function to merge all the smaller dictionaries into one and replicate this same set for every\n# small item group at the lower dict level.\ndef merge_small_groups(group_dict):\n    for index_name in group_dict:\n        for product_group_name in group_dict[index_name]:\n            for section_name in group_dict[index_name][product_group_name]:\n                small_section_list = []\n                small_section_item_list = []\n\n                for product_type_name in group_dict[index_name][product_group_name][section_name]:\n                    curr_group = group_dict[index_name][product_group_name][section_name][product_type_name]\n                    if len(curr_group) < neg_sample_num + 1:\n                        small_section_list.append(product_type_name)\n                        small_section_item_list = small_section_item_list + curr_group\n\n                for section in small_section_list:\n                    group_dict[index_name][product_group_name][section_name][section] = small_section_item_list\n                    \n    return group_dict\n                    \ngroup_dict_merged = merge_small_groups(group_dict)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:38:53.082269Z","iopub.execute_input":"2022-04-24T23:38:53.082807Z","iopub.status.idle":"2022-04-24T23:38:53.091038Z","shell.execute_reply.started":"2022-04-24T23:38:53.082770Z","shell.execute_reply":"2022-04-24T23:38:53.090366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get reverse dictoinary address book -> article id -> group\ndef create_article_group_dict():\n    item_group_dict = {}\n    groups = dfi.groupby(['index_name', 'product_group_name', 'section_name', 'product_type_name']).groups\n    for key, values in groups.items():\n        for vl in values:\n            item_group_dict[vl] = key\n    \n    return item_group_dict\n\nitem_group_dict = create_article_group_dict()    ","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:38:56.439998Z","iopub.execute_input":"2022-04-24T23:38:56.440508Z","iopub.status.idle":"2022-04-24T23:38:56.743374Z","shell.execute_reply.started":"2022-04-24T23:38:56.440471Z","shell.execute_reply":"2022-04-24T23:38:56.742656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define train and test sets","metadata":{}},{"cell_type":"code","source":"# the train dataset is df\ntrain_set = (df.loc[df.t_dat >= pd.Timestamp(train_begin_date)]).loc[df.t_dat <= pd.Timestamp(train_end_date)]\nval_set = (df.loc[df.t_dat > pd.Timestamp(train_end_date)]).loc[df.t_dat <= pd.Timestamp(val_end_date)]\n\n# drop columns we do not need\ntrain_set = train_set[['customer_id', 'article_id', 'price']]\nval_set = val_set[['t_dat', 'customer_id', 'article_id', 'price']]\n\n# convert prices to implicit feedback (1 for interaction)\ntrain_set.loc[:, 'price'] = 1\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-25T02:42:09.770934Z","iopub.execute_input":"2022-04-25T02:42:09.771182Z","iopub.status.idle":"2022-04-25T02:42:11.463562Z","shell.execute_reply.started":"2022-04-25T02:42:09.771153Z","shell.execute_reply":"2022-04-25T02:42:11.462771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-25T02:42:25.058635Z","iopub.execute_input":"2022-04-25T02:42:25.059408Z","iopub.status.idle":"2022-04-25T02:42:25.065507Z","shell.execute_reply.started":"2022-04-25T02:42:25.059353Z","shell.execute_reply":"2022-04-25T02:42:25.064739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make training data more memory efficient","metadata":{}},{"cell_type":"code","source":"# use more memory efficient ids\nid_to_index_dict = dict(zip(dfu[\"customer_id\"], dfu.index))\nindex_to_id_dict = dict(zip(dfu.index, dfu[\"customer_id\"]))\nid2inxArt = dict(zip(dfi[\"article_id\"], dfi.index))\ninx2idArt = dict(zip(dfi.index, dfi[\"article_id\"]))\n\ntrain_set[\"customer_id\"] = train_set[\"customer_id\"].map(id_to_index_dict)\ntrain_set[\"customer_id\"]= train_set[\"customer_id\"].astype('int32')\ntrain_set[\"article_id\"] = train_set[\"article_id\"].map(id2inxArt)\n\n# for switching back for submission use:\n# sub[\"customer_id\"] = sub[\"customer_id\"].map(index_to_id_dict)\n\n# create needed constants\nall_itemIds = dfi['article_id'].map(id2inxArt)\nnum_customers = dfu['customer_id'].unique().shape[0]\nnum_items = dfi['article_id'].unique().shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-25T02:42:18.171676Z","iopub.execute_input":"2022-04-25T02:42:18.172333Z","iopub.status.idle":"2022-04-25T02:42:23.691023Z","shell.execute_reply.started":"2022-04-25T02:42:18.172286Z","shell.execute_reply":"2022-04-25T02:42:23.690278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define data processing class for training data and create training data object","metadata":{}},{"cell_type":"code","source":"# Goal: develope negative sampling procedure\n# plan: for now just add neg examples if possible.\ndef select_neg_items(article_id, u, user_item_set):\n    neg_item_list = []\n    \n    # get dictionary address from article id\n    group_address = item_group_dict[article_id]\n    # get item set for potential neg sampels\n    group = group_dict_merged[group_address[0]][group_address[1]][group_address[2]][group_address[3]]\n\n    curr_neg_sample_num = neg_sample_num\n    if len(group) <= neg_sample_num:\n        if len(group) != 0:\n            curr_neg_sample_num = len(group) - 1\n        else:\n            curr_neg_sample_num = 0\n            \n    for _ in range(curr_neg_sample_num):\n        to_add = True\n        \n        negative_item = np.random.choice(group)\n        \n        index = 0\n        while ((u, negative_item) in user_item_set) or (negative_item in neg_item_list):\n            index += 1\n            negative_item = np.random.choice(group)\n            if index > len(group) * 3:\n                to_add = False\n#                 print(\"Couldn't find a neg item.\")\n                break\n        \n        if to_add:\n            neg_item_list.append(negative_item)\n       \n    return neg_item_list\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:40:42.834542Z","iopub.execute_input":"2022-04-24T23:40:42.834843Z","iopub.status.idle":"2022-04-24T23:40:42.845780Z","shell.execute_reply.started":"2022-04-24T23:40:42.834810Z","shell.execute_reply":"2022-04-24T23:40:42.845036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    \"\"\"Transactions PyTorch Dataset for Training\n    \n    Args:\n        ratings (pd.DataFrame): Dataframe containing the transactions data\n        all_itemIds (list): List containing all article ids\n    \n    \"\"\"\n\n    def __init__(self, ratings, all_itemIds):\n        self.users, self.items, self.labels = self.get_dataset(ratings, all_itemIds)\n\n    def __len__(self):\n        return len(self.users)\n  \n    def __getitem__(self, idx):\n        return self.users[idx], self.items[idx], self.labels[idx]\n\n    def get_dataset(self, df, all_itemIds):\n        users, items, labels = [], [], []\n        user_item_set = set(zip(df['customer_id'], df['article_id']))\n\n        # simple negative sampling - choose four random articles as negative samples.\n        # TODO try a different approach here\n        print(f'The length of the set is {len(user_item_set)}')\n        index = 0\n        for u, i in user_item_set:\n            index += 1\n            if index % 100000 == 0:\n                print(f'The index is {index}')\n            users.append(u)\n            items.append(i)\n            labels.append(1)\n            \n            neg_item_list = select_neg_items(i, u, user_item_set)\n            for negative_item in neg_item_list:\n                users.append(u)\n                items.append(negative_item)\n                labels.append(0)\n\n        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:40:46.637047Z","iopub.execute_input":"2022-04-24T23:40:46.637725Z","iopub.status.idle":"2022-04-24T23:40:46.647970Z","shell.execute_reply.started":"2022-04-24T23:40:46.637689Z","shell.execute_reply":"2022-04-24T23:40:46.647289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndata = TrainDataset(train_set, all_itemIds)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-25T02:42:39.505636Z","iopub.execute_input":"2022-04-25T02:42:39.506229Z","iopub.status.idle":"2022-04-25T03:14:56.679907Z","shell.execute_reply.started":"2022-04-25T02:42:39.506181Z","shell.execute_reply":"2022-04-25T03:14:56.679112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(data, \"/kaggle/working/dataset-2020.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T03:15:19.508478Z","iopub.execute_input":"2022-04-25T03:15:19.508739Z","iopub.status.idle":"2022-04-25T03:15:20.934857Z","shell.execute_reply.started":"2022-04-25T03:15:19.508709Z","shell.execute_reply":"2022-04-25T03:15:20.934069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data = torch.load(\"/kaggle/working/dataset-2020.pt\")\nnew_data[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:08:38.124235Z","iopub.execute_input":"2022-04-20T05:08:38.124829Z","iopub.status.idle":"2022-04-20T05:08:38.80654Z","shell.execute_reply.started":"2022-04-20T05:08:38.12478Z","shell.execute_reply":"2022-04-20T05:08:38.805312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the model class","metadata":{}},{"cell_type":"code","source":"class NCF(pl.LightningModule):\n    \"\"\" Neural Collaborative Filtering (NCF)\n    \n        Args:\n            num_users (int): Number of unique users\n            num_items (int): Number of unique items\n            ratings (pd.DataFrame): Dataframe containing the movie ratings for training\n            all_movieIds (list): List containing all movieIds (train + test)\n    \"\"\"\n    \n    def __init__(self, num_users, num_items, ratings, all_itemIds):\n        super().__init__()\n        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=8)\n        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=8)\n        self.fc1 = nn.Linear(in_features=16, out_features=64)\n        self.fc2 = nn.Linear(in_features=64, out_features=32)\n        self.output = nn.Linear(in_features=32, out_features=1)\n        self.ratings = ratings\n        self.all_itemIds = all_itemIds\n        \n    def forward(self, user_input, item_input):\n        \n        # Pass through embedding layers\n        user_embedded = self.user_embedding(user_input)\n        item_embedded = self.item_embedding(item_input)\n\n        # Concat the two embedding layers\n        vector = torch.cat([user_embedded, item_embedded], dim=-1)\n\n        # Pass through dense layer\n        vector = nn.ReLU()(self.fc1(vector))\n        vector = nn.ReLU()(self.fc2(vector))\n\n        # Output layer\n        pred = nn.Sigmoid()(self.output(vector))\n\n        return pred\n    \n    def training_step(self, batch, batch_idx):\n        user_input, item_input, labels = batch\n        predicted_labels = self(user_input, item_input)\n        loss = nn.BCELoss()(predicted_labels, labels.view(-1, 1).float())\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters())\n\n    def train_dataloader(self):\n        return DataLoader(data,\n                          batch_size=512, num_workers=2) # increased worker number","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-24T23:50:51.520984Z","iopub.execute_input":"2022-04-24T23:50:51.521259Z","iopub.status.idle":"2022-04-24T23:50:51.532595Z","shell.execute_reply.started":"2022-04-24T23:50:51.521208Z","shell.execute_reply":"2022-04-24T23:50:51.531896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"# instantiate the model\nmodel = NCF(num_customers, num_items, train_set, all_itemIds)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T03:19:44.378647Z","iopub.execute_input":"2022-04-25T03:19:44.378904Z","iopub.status.idle":"2022-04-25T03:19:44.471659Z","shell.execute_reply.started":"2022-04-25T03:19:44.378874Z","shell.execute_reply":"2022-04-25T03:19:44.470960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = pl.Trainer(\n    enable_checkpointing=True,\n    max_epochs=5, \n    gpus=1, \n    reload_dataloaders_every_epoch=True, \n    progress_bar_refresh_rate=50, \n    logger=False,\n    default_root_dir=\"/kaggle/working\"\n)\n\ntrainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T03:19:56.797126Z","iopub.execute_input":"2022-04-25T03:19:56.797405Z","iopub.status.idle":"2022-04-25T04:10:33.994674Z","shell.execute_reply.started":"2022-04-25T03:19:56.797374Z","shell.execute_reply":"2022-04-25T04:10:33.993959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the model as a file - do not forget to download","metadata":{}},{"cell_type":"code","source":"trainer.save_checkpoint(\"/kaggle/working/curr_checkpoint-2020.ckpt\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:14:44.521386Z","iopub.execute_input":"2022-04-25T04:14:44.522195Z","iopub.status.idle":"2022-04-25T04:14:44.746310Z","shell.execute_reply.started":"2022-04-25T04:14:44.522156Z","shell.execute_reply":"2022-04-25T04:14:44.745464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# export the model\nfilepath = '/kaggle/working/model.pt'\ntorch.save(model.state_dict(), filepath)\n\n## code to load the model\n# loaded_model = NCF(num_customers, num_items, train_set, all_itemIds)\n# loaded_model.load_state_dict(torch.load(filepath))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"\"\nmodel = NCF.load_from_checkpoint(PATH)\ntrainer = pl.Trainer()\ntrainer.fit(model, ckpt_path=PATH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create potential predictions set for each customer in validation set","metadata":{}},{"cell_type":"code","source":"val_set = (df.loc[df.t_dat >= pd.Timestamp(train_end_date)]).loc[df.t_dat <= pd.Timestamp(val_end_date)]\nval_set = val_set[['t_dat', 'customer_id', 'article_id', 'price']]\nval_set[\"customer_id\"] = val_set[\"customer_id\"].map(id_to_index_dict)\nval_set[\"customer_id\"]= val_set[\"customer_id\"].astype('int32')\nval_set[\"article_id\"] = val_set[\"article_id\"].map(id2inxArt)\nval_set = val_set[['t_dat','customer_id','article_id']]\nval_set.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:16:47.074930Z","iopub.execute_input":"2022-04-25T04:16:47.075172Z","iopub.status.idle":"2022-04-25T04:16:48.458814Z","shell.execute_reply.started":"2022-04-25T04:16:47.075139Z","shell.execute_reply":"2022-04-25T04:16:48.458143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FIND PREVIOUS PURCHASES\n# the train dataset is df\ntrain_set = (df.loc[df.t_dat >= pd.Timestamp(train_begin_date)]).loc[df.t_dat <= pd.Timestamp(train_end_date)]\n\n# drop columns we do not need\ntrain_set = train_set[['t_dat', 'customer_id', 'article_id', 'price']]\n\n# use more memory efficient ids\ntrain_set[\"customer_id\"] = train_set[\"customer_id\"].map(id_to_index_dict)\ntrain_set[\"customer_id\"]= train_set[\"customer_id\"].astype('int32')\ntrain_set[\"article_id\"] = train_set[\"article_id\"].map(id2inxArt)\n\ntrain_set.head()\n\ntmp = train_set.groupby('customer_id').t_dat.max().reset_index()\ntmp.columns = ['customer_id','max_dat']\ntrain_set = train_set.merge(tmp,on=['customer_id'],how='left')\ntrain_set['diff_dat'] = (train_set.max_dat - train_set.t_dat).dt.days\n\ntrain_set = train_set.loc[train_set['diff_dat']<=days_max_diff] # UNCOMMENT\nprint('Train shape:',train_set.shape)\n\n# only leave customers in the validation set\nUSERS = val_set['customer_id'].unique()\ntrain_set = train_set[train_set['customer_id'].isin(USERS)]\ntrain_set.head()\ntrain_set = train_set.drop_duplicates(['customer_id','article_id'])\ntrain_set.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:17:10.951696Z","iopub.execute_input":"2022-04-25T04:17:10.951953Z","iopub.status.idle":"2022-04-25T04:17:19.582750Z","shell.execute_reply.started":"2022-04-25T04:17:10.951925Z","shell.execute_reply":"2022-04-25T04:17:19.582020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set[['customer_id', 'article_id']].groupby('customer_id').count().mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T18:36:32.934148Z","iopub.execute_input":"2022-04-22T18:36:32.934404Z","iopub.status.idle":"2022-04-22T18:36:32.992829Z","shell.execute_reply.started":"2022-04-22T18:36:32.934371Z","shell.execute_reply":"2022-04-22T18:36:32.991983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add items purchased together\n# USE PANDAS TO MAP COLUMN WITH DICTIONARY\npairs = np.load('../input/hmitempairs/pairs_cudf.npy',allow_pickle=True).item()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:01:12.550560Z","iopub.execute_input":"2022-04-25T00:01:12.550865Z","iopub.status.idle":"2022-04-25T00:01:12.685407Z","shell.execute_reply.started":"2022-04-25T00:01:12.550830Z","shell.execute_reply":"2022-04-25T00:01:12.684635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert article ids\n# map pairs to reduced article id, then map train set to the mapped pairs.\npairs_converted = {}\nfor key in pairs:\n    # change the key into article numeric id\n    article_id = '0' + str(key)\n    inx = id2inxArt[article_id]\n    value = id2inxArt['0' + str(pairs[key])]\n    pairs_converted[inx] = value\n\ntrain_set['article_id2'] = train_set.article_id.map(pairs_converted)\n\n# RECOMMENDATION OF PAIRED ITEMS\ntrain2 = train_set[['customer_id','article_id2']].copy()\ntrain2 = train2.loc[train2.article_id2.notnull()]\ntrain2 = train2.drop_duplicates(['customer_id','article_id2'])\ntrain2 = train2.rename({'article_id2':'article_id'},axis=1)\ntrain2.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:17:52.442488Z","iopub.execute_input":"2022-04-25T04:17:52.442772Z","iopub.status.idle":"2022-04-25T04:17:52.782807Z","shell.execute_reply.started":"2022-04-25T04:17:52.442742Z","shell.execute_reply":"2022-04-25T04:17:52.781922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2.groupby('customer_id').count().mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T18:37:12.722641Z","iopub.execute_input":"2022-04-22T18:37:12.722927Z","iopub.status.idle":"2022-04-22T18:37:12.773591Z","shell.execute_reply.started":"2022-04-22T18:37:12.722889Z","shell.execute_reply":"2022-04-22T18:37:12.772794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CONCATENATE PAIRED ITEM RECOMMENDATION AFTER PREVIOUS PURCHASED RECOMMENDATIONS\ntrain_set = train_set[['customer_id','article_id']]\ntrain_set = pd.concat([train_set,train2],axis=0,ignore_index=True)\ntrain_set.article_id = train_set.article_id.astype('int32')\ntrain_set = train_set.drop_duplicates(['customer_id','article_id'])","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:17:59.687617Z","iopub.execute_input":"2022-04-25T04:17:59.688081Z","iopub.status.idle":"2022-04-25T04:17:59.736997Z","shell.execute_reply.started":"2022-04-25T04:17:59.688043Z","shell.execute_reply":"2022-04-25T04:17:59.736237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set.groupby('customer_id').count().mean()\ntrain_set = train_set.reset_index(drop=True)\n\n# create dictionary -> customer: list of items\ntrain_preds = train_set.groupby(['customer_id'])\npred_list_dict = {}\n\nfor key in train_preds.groups.keys():\n    value = list(train_preds.get_group(key)['article_id'])\n    pred_list_dict[key] = value\n    \npred_list_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/h-and-m-personalized-fashion-recommendations/'\ncsv_train = f'{data_path}transactions_train.csv'\n\ndf = pd.read_csv(csv_train, dtype={'article_id': str}, parse_dates=['t_dat'])\n\n# RECOMMENT LAST MONTH'S MOST POPULAR ITEMS\npop_train = (df.loc[df.t_dat >= pd.Timestamp(one_month_before_val_date)]).loc[df.t_dat <= pd.Timestamp(train_end_date)]\npop_items = dict(pop_train['article_id'].value_counts()[:num_pop_items]).keys()\npop_items_converted = []\nfor item in pop_items:\n    pop_items_converted.append(id2inxArt[item])\n    \npop_items_converted","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:20:57.745420Z","iopub.execute_input":"2022-04-25T04:20:57.745699Z","iopub.status.idle":"2022-04-25T04:21:51.052343Z","shell.execute_reply.started":"2022-04-25T04:20:57.745669Z","shell.execute_reply":"2022-04-25T04:21:51.051655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make prediction","metadata":{}},{"cell_type":"code","source":"# create predictions for all the customers in the validation set\n# return submission-formatted dataframe\ndef make_predictions(val_subm_set, item_candidate_list, pop_items):\n    \"\"\"\n    customer_list: a list of customer ids as strings for which to make predictions\n    \"\"\"\n    # get validation set and turn int test_user_item_set\n\n    predictions = val_subm_set.copy(deep=True)\n    customer_list = list(val_subm_set['customer_id'])\n\n    index = 0\n    # tqdm is a progress bar\n    for index, u in tqdm(enumerate(customer_list)):\n        # get predictions\n        try:\n            test_items = item_candidate_list[u] + pop_items\n        except:\n            test_items = pop_items\n            \n        test_items_length = len(test_items)\n            \n        predicted_labels = np.squeeze(model(torch.tensor([u]*test_items_length), \n                                        torch.tensor(test_items)).detach().numpy())\n    \n        top12_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:12].tolist()]\n    \n        # convert predictions to article ids and make them a string\n        top_articles_ids = list(map(inx2idArt.get, top12_items))\n    \n        top_articles_ids_str = ' '.join([str(k) for k in top_articles_ids])\n    \n        # add row to data frame\n        predictions.iloc[index] = [index_to_id_dict[u], top_articles_ids_str]\n    \n#         index += 1\n#         if index > 10:\n#             break\n    \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-25T02:25:11.840532Z","iopub.execute_input":"2022-04-25T02:25:11.841229Z","iopub.status.idle":"2022-04-25T02:25:11.850072Z","shell.execute_reply.started":"2022-04-25T02:25:11.841180Z","shell.execute_reply":"2022-04-25T02:25:11.849052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert test set into submission like format for evaluatoin","metadata":{}},{"cell_type":"code","source":"# convert val set to submission file equilvalent\nvalid = val_set.groupby('customer_id').article_id.apply(list).reset_index()\nvalid = valid.rename({'article_id':'prediction'},axis=1)\nvalid['prediction'] =\\\n    valid.prediction.apply(lambda x: ' '.join([str(k) for k in x]))\nvalid.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:22:23.438931Z","iopub.execute_input":"2022-04-25T04:22:23.439216Z","iopub.status.idle":"2022-04-25T04:22:24.860365Z","shell.execute_reply.started":"2022-04-25T04:22:23.439186Z","shell.execute_reply":"2022-04-25T04:22:24.859428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:22:32.578017Z","iopub.execute_input":"2022-04-25T04:22:32.578292Z","iopub.status.idle":"2022-04-25T04:22:32.584152Z","shell.execute_reply.started":"2022-04-25T04:22:32.578261Z","shell.execute_reply":"2022-04-25T04:22:32.583287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = make_predictions(valid, pred_list_dict, pop_items_converted)\n                               \npredictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:22:35.638410Z","iopub.execute_input":"2022-04-25T04:22:35.639186Z","iopub.status.idle":"2022-04-25T04:28:01.368683Z","shell.execute_reply.started":"2022-04-25T04:22:35.639150Z","shell.execute_reply":"2022-04-25T04:28:01.367994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate on test set","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/306007\n# https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n\n\ndef apk(actual, predicted, k=10):\n    \"\"\"\n    Computes the average precision at k.\n    This function computes the average prescision at k between two lists of\n    items.\n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n    \"\"\"\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    # remove this case in advance\n    # if not actual:\n    #     return 0.0\n\n    return score / min(len(actual), k)\n\n\ndef mapk(actual, predicted, k=10):\n    \"\"\"\n    Computes the mean average precision at k.\n    This function computes the mean average prescision at k between two lists\n    of lists of items.\n    Parameters\n    ----------\n    actual : list\n             A list of lists of elements that are to be predicted \n             (order doesn't matter in the lists)\n    predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n    \"\"\"\n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2022-04-25T01:09:05.835700Z","iopub.execute_input":"2022-04-25T01:09:05.836300Z","iopub.status.idle":"2022-04-25T01:09:05.845002Z","shell.execute_reply.started":"2022-04-25T01:09:05.836260Z","shell.execute_reply":"2022-04-25T01:09:05.844266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Calculate MAP@12","metadata":{}},{"cell_type":"code","source":"predictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:29:48.904447Z","iopub.execute_input":"2022-04-25T04:29:48.904716Z","iopub.status.idle":"2022-04-25T04:29:48.913559Z","shell.execute_reply.started":"2022-04-25T04:29:48.904686Z","shell.execute_reply":"2022-04-25T04:29:48.912742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/h-and-m-personalized-fashion-recommendations/'\ncsv_train = f'{data_path}transactions_train.csv'\ndf = pd.read_csv(csv_train, dtype={'article_id': str}, parse_dates=['t_dat'])\nval_set = (df.loc[df.t_dat >= pd.Timestamp(train_end_date)]).loc[df.t_dat <= pd.Timestamp(val_end_date)]\nval_set = val_set[['customer_id','article_id']]\n\n# convert val set to submission file equilvalent\nvalid = val_set.groupby('customer_id').article_id.apply(list).reset_index()\nvalid = valid.rename({'article_id':'prediction'},axis=1)\nvalid['prediction'] =\\\n    valid.prediction.apply(lambda x: ' '.join([str(k) for k in x]))\nvalid.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:30:43.524620Z","iopub.execute_input":"2022-04-25T04:30:43.525358Z","iopub.status.idle":"2022-04-25T04:31:25.456457Z","shell.execute_reply.started":"2022-04-25T04:30:43.525315Z","shell.execute_reply":"2022-04-25T04:31:25.455765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate MAP@12\n# valid_test = valid.head(7)\n# predictions_test = predictions.head(7)\n\npred_arranged = predictions.set_index('customer_id').loc[valid.customer_id].reset_index()\nevaluation = mapk( valid.prediction.str.split(), pred_arranged.prediction.str.split(), k=12)\nprint(f\"Test set results: MAP@12 = {evaluation}.\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:31:41.176183Z","iopub.execute_input":"2022-04-25T04:31:41.176523Z","iopub.status.idle":"2022-04-25T04:31:41.670233Z","shell.execute_reply.started":"2022-04-25T04:31:41.176487Z","shell.execute_reply":"2022-04-25T04:31:41.669414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create submission file and submit","metadata":{}},{"cell_type":"code","source":"# create submission file\nsub = make_predictions(list(df_sub['customer_id']))\n\n# submit competition\n# To be done\n\n","metadata":{},"execution_count":null,"outputs":[]}]}